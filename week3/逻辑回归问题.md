#### 过拟合问题
如下几个图可以直观的表示在线性回归问题中，过拟合，欠拟合，和正确拟合
![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合1.PNG?raw=true)

分类问题中同样存在这样的问题：
![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合2.PNG?raw=true)

防止过拟合的几种办法：
1. 减少特征
2. 增加数据量
3. 正则化(Regularized)

#### 正则化

正则化代价函数 ：

L1正则化：

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合4.PNG?raw=true)

L2正则化：

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合3.PNG?raw=true)

#### 逻辑回归函数(Sigmoid/Logistic Function)
我们定义逻辑回归的预测函数为 ℎ𝜃(𝑥) = 𝑔(𝜃𝑇𝑋)
其中g（x）函数是sigmoid函数
![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合5.PNG?raw=true)



![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合6.PNG?raw=true)


> 0.5可以作为分类的边界

    当z>=0的时候g(z) >=0.5
    当𝜃^𝑇𝑋>=0的时候g(𝜃^𝑇𝑋) >= 0.5

    当z =< 0的时候g(z) <= 0.5
    当𝜃^𝑇𝑋 =< 0的时候g(𝜃^𝑇𝑋) =< 0.5

#### 决策边界
如图 该线性函数 当 -3 + x1 + x2 >= 0 则 y = 1

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合7.PNG?raw=true)


当函数为非线性时

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/拟合8.PNG?raw=true)


#### 逻辑回归的代价函数
逻辑回归与线性回归不同，其代价函数分段来表示：

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑回归1.PNG?raw=true)

当 y = 1, h(x) = 1时 ，cost = 0

当 y = 1, h(x) = 0时 ，cost = 无穷

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价1.PNG?raw=true)

当 y = 0, h(x) = 1时 ，cost = 无穷

当 y = 0, h(x) = 0时 ，cost = 0

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价2.PNG?raw=true)

将其分段写成如下的形式，y = 1 或者当 y = 0 时都符合上面的分段情况

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价3.PNG?raw=true)

然后对该代价函数进行梯度下降

gradient descent:

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价4.PNG?raw=true)

然后不断求导迭代，下面是对其进行求导的过程

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价5.PNG?raw=true)

#### 逻辑回归正则化
逻辑回归也可以进行正则化，只要在普通的逻辑回归函数后面加正则化部分即可

普通的逻辑回归代价函数：

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑代价4.PNG?raw=true)

下面对其进行L2型正则化

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑正则化1.PNG?raw=true)

然后后对该函数进行求导：

![image](https://github.com/jccjd/Coursera-Machine-Learning/blob/master/week3/image/逻辑正则化2.PNG?raw=true)



